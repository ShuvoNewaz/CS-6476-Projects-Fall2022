{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Project 3: Camera Calibration and Fundamental Matrix Estimation with RANSAC](https://www.cc.gatech.edu/~hays/compvision/proj3)\n",
    "\n",
    "\n",
    "(1) Camera Projection Matrix  \n",
    "(2) Fundamental Matrix Estimation  \n",
    "(3) Fundamental Matrix with RANSAC   \n",
    "(4) Comparison of the results from (2) and (3)   \n",
    "(5) Using F-Matrix Estimation w/ RANSAC for Visual Odometry   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from vision.utils import (\n",
    "    verify,\n",
    "    evaluate_points,\n",
    "    visualize_points,\n",
    "    visualize_points_image,\n",
    "    plot3dview,\n",
    "    load_image,\n",
    "    draw_epipolar_lines,\n",
    "    get_matches,\n",
    "    show_correspondence2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Camera Projection Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "img_path = \"data/CCB_GaTech/pic_a.jpg\"\n",
    "points_2d = np.loadtxt(\"data/CCB_GaTech/pts2d-norm-pic_a.txt\")\n",
    "points_3d = np.loadtxt(\"data/CCB_GaTech/pts3d-norm.txt\")\n",
    "\n",
    "# (Optional) Uncomment these four lines once you have your code working with the easier, normalized points above.\n",
    "# points_2d = np.loadtxt('../data/CCB_GaTech/pts2d-pic_b.txt')\n",
    "# points_3d = np.loadtxt('../data/CCB_GaTech/pts3d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the projection matrix given corresponding 2D & 3D points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_projection_matrix import (\n",
    "    calculate_projection_matrix,\n",
    "    calculate_camera_center,\n",
    ")\n",
    "\n",
    "from tests.test_part1 import (\n",
    "    test_projection,\n",
    "    test_calculate_projection_matrix,\n",
    "    test_calculate_camera_center,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"projection():\", verify(test_projection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "M = calculate_projection_matrix(points_2d, points_3d)\n",
    "print(\"The projection matrix is\\n\", M)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(M, points_2d, points_3d)\n",
    "print(\"The total residual is {:f}\".format(residual))\n",
    "plt.figure()\n",
    "plt.imshow(load_image(img_path))\n",
    "visualize_points(points_2d, projected_2d_pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"calculate_projection_matrix():\", verify(test_calculate_projection_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the camera center using M found from the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = calculate_camera_center(M)\n",
    "print(\n",
    "    \"The estimated location of the camera is <{:.4f}, {:.4f}, {:.4f}>\".format(*center)\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(load_image(img_path))\n",
    "ax = plot3dview(points_3d, center)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_calculate_camera_center():\", verify(test_calculate_camera_center))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Camera Calibration for Argoverse image data\n",
    "We'll now estimate the position of a camera mounted on an autonomous vehicle, using data from Argoverse. We'll use images from the \"ring front center\" camera, which faces forward.\n",
    "\n",
    "\n",
    "<img src=\"https://www.argoverse.org/assets/images/reference_images/O2V4_vehicle_annotation.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Argoverse Scene 3\n",
    "img_path = \"data/argoverse_log_d60558d2_pair3/pic3.jpg\"\n",
    "points_2d = np.loadtxt(\"data/argoverse_log_d60558d2_pair3/points_2d.txt\")\n",
    "points_3d = np.loadtxt(\"data/argoverse_log_d60558d2_pair3/points_3d.txt\")\n",
    "# # # Argoverse Scene 2\n",
    "# img_path = '../data/argoverse_log_d60558d2_pair2/pic2.jpg'\n",
    "# points_2d = np.loadtxt('../data/argoverse_log_d60558d2_pair2/points_2d.txt')\n",
    "# points_3d = np.loadtxt('../data/argoverse_log_d60558d2_pair2/points_3d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = calculate_projection_matrix(points_2d, points_3d)\n",
    "print(\"The projection matrix is\\n\", M)\n",
    "\n",
    "[projected_2d_pts, residual] = evaluate_points(M, points_2d, points_3d)\n",
    "print(\"The total residual is {:f}\".format(residual))\n",
    "plt.figure()\n",
    "plt.imshow(load_image(img_path))\n",
    "visualize_points(points_2d, projected_2d_pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these 2d-3d pairs, the \"world\" frame is defined as the \"ego-vehicle\" frame, where the origin is at the center of the back axle of the vehicle.\n",
    "\n",
    "Thus, if your camera center estimate is correct, it should tell you how far to move forward (+x) and how far to move left (+y) and move up (+z) to reach teh camera's position.\n",
    "\n",
    "\n",
    "The \"egovehicle\" coordinate system and \"camera\" coordinate system:\n",
    "<img width=\"300\"  src=\"https://user-images.githubusercontent.com/16724970/108759169-034e6180-751a-11eb-8a06-fbe344f1ee68.png\">\n",
    "<img width=\"300\" src=\"https://user-images.githubusercontent.com/16724970/108759182-06495200-751a-11eb-8162-8b17f9cdee4b.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "center = calculate_camera_center(M)\n",
    "print(\n",
    "    \"The estimated location of the camera is <{:.4f}, {:.4f}, {:.4f}>\".format(*center)\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(load_image(img_path))\n",
    "ax = plot3dview(points_3d, center)\n",
    "ax.view_init(elev=15, azim=180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fundamental Matrix Estimation\n",
    "We'll now solve for the Fundamental Matrix by implementing [Hartley's 8-Point algorithm](https://www.cse.unr.edu/~bebis/CS485/Handouts/hartley.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part2_fundamental_matrix import estimate_fundamental_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "points_2d_pic_a = np.loadtxt(\"data/CCB_GaTech/pts2d-pic_a.txt\")\n",
    "points_2d_pic_b = np.loadtxt(\"data/CCB_GaTech/pts2d-pic_b.txt\")\n",
    "img_a = load_image(\"data/CCB_GaTech/pic_a.jpg\")\n",
    "img_b = load_image(\"data/CCB_GaTech/pic_b.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimate fundamental matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part2 import (\n",
    "    test_estimate_fundamental_matrix,\n",
    "    test_normalize_points,\n",
    "    test_unnormalize_F,\n",
    ")\n",
    "\n",
    "print(\"test_estimate_fundamental_matrix():\", verify(test_estimate_fundamental_matrix))\n",
    "print(\"test_normalize_points():\", verify(test_normalize_points))\n",
    "print(\"test_unnormalize_F():\", verify(test_unnormalize_F))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F = estimate_fundamental_matrix(points_2d_pic_a, points_2d_pic_b)\n",
    "\n",
    "# Draw epipolar lines using the fundamental matrix\n",
    "draw_epipolar_lines(F, img_a, img_b, points_2d_pic_a, points_2d_pic_b, figsize=(13, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Fundamental Matrix with RANSAC (Szeliski 6.1.4)\n",
    "\n",
    "**Mount Rushmore**: This pair is easy, and most of the initial matches are correct. The base fundamental matrix estimation without coordinate normalization will work fine with RANSAC. \n",
    "\n",
    "**Notre Dame**: This pair is difficult because the keypoints are largely on the same plane. Still, even an inaccurate fundamental matrix can do a pretty good job of filtering spurious matches.  \n",
    "\n",
    "**Gaudi**: This pair is difficult and doesn't find many correct matches unless you run at high resolution, but that will lead to tens of thousands of SIFT features, which will be somewhat slow to process. Normalizing the coordinates seems to make this pair work much better.  \n",
    "\n",
    "**Woodruff**: This pair has a clearer relationship between the cameras (they are converging and have a wide baseline between them). The estimated fundamental matrix is less ambiguous and you should get epipolar lines qualitatively similar to part 2 of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part3_ransac import (\n",
    "    calculate_num_ransac_iterations,\n",
    "    ransac_fundamental_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "# Mount Rushmore\n",
    "# pic_a = load_image('../data/Mount_Rushmore/9193029855_2c85a50e91_o.jpg'); scale_a = 0.25\n",
    "# pic_b = load_image('../data/Mount_Rushmore/7433804322_06c5620f13_o.jpg'); scale_b = 0.37\n",
    "# n_feat = 5e4\n",
    "\n",
    "# Notre Dame\n",
    "pic_a = load_image(\"data/Notre_Dame/921919841_a30df938f2_o.jpg\")\n",
    "scale_a = 0.5\n",
    "pic_b = load_image(\"data/Notre_Dame/4191453057_c86028ce1f_o.jpg\")\n",
    "scale_b = 0.5\n",
    "n_feat = 4e3\n",
    "\n",
    "# Gaudi\n",
    "# pic_a = load_image('../data/Episcopal_Gaudi/3743214471_1b5bbfda98_o.jpg'); scale_a = 0.8\n",
    "# pic_b = load_image('../data/Episcopal_Gaudi/4386465943_8cf9776378_o.jpg'); scale_b = 1.0\n",
    "# n_feat = 2e4\n",
    "\n",
    "# Woodruff\n",
    "# pic_a = load_image('../data/Woodruff_Dorm/wood1.jpg'); scale_a = 0.65\n",
    "# pic_b = load_image('../data/Woodruff_Dorm/wood2.jpg'); scale_b = 0.65\n",
    "# n_feat = 5e4\n",
    "\n",
    "pic_a = cv2.resize(pic_a, None, fx=scale_a, fy=scale_a)\n",
    "pic_b = cv2.resize(pic_b, None, fx=scale_b, fy=scale_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finds matching points in the two images using OpenCV's implementation of SIFT.\n",
    "# There can still be many spurious matches, though.\n",
    "points_2d_pic_a, points_2d_pic_b = get_matches(pic_a, pic_b, n_feat)\n",
    "print(\"Found {:d} possibly matching features\".format(len(points_2d_pic_a)))\n",
    "match_image = show_correspondence2(\n",
    "    pic_a,\n",
    "    pic_b,\n",
    "    points_2d_pic_a[:, 0],\n",
    "    points_2d_pic_a[:, 1],\n",
    "    points_2d_pic_b[:, 0],\n",
    "    points_2d_pic_b[:, 1],\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(match_image)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the Fundamental Matrix using RANSAC\n",
    "Compare your results on the Notre Dame image pair below to your results from Project 2. How accurate do the point correspondences look now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "F, matched_points_a, matched_points_b = ransac_fundamental_matrix(\n",
    "    points_2d_pic_a, points_2d_pic_b\n",
    ")\n",
    "\n",
    "# Draw the epipolar lines on the images and corresponding matches\n",
    "match_image = show_correspondence2(\n",
    "    pic_a,\n",
    "    pic_b,\n",
    "    matched_points_a[:, 0],\n",
    "    matched_points_a[:, 1],\n",
    "    matched_points_b[:, 0],\n",
    "    matched_points_b[:, 1],\n",
    ")\n",
    "plt.figure()\n",
    "plt.imshow(match_image)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_epipolar_lines(\n",
    "    F, pic_a, pic_b, matched_points_a, matched_points_b, figsize=(12, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part3 import (\n",
    "    test_calculate_num_ransac_iterations,\n",
    "    test_ransac_fundamental_matrix,\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"test_calculate_num_ransac_iterations():\",\n",
    "    verify(test_calculate_num_ransac_iterations),\n",
    ")\n",
    "print(\"test_ransac_fundamental_matrix():\", verify(test_ransac_fundamental_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Performance Comparison\n",
    "We'll now test the quality of Fundamental matrices we can compute with and without RANSAC on an image pair from the [Argoverse](https://www.argoverse.org/) autonomous driving dataset. Does RANSAC improve the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part3_ransac import (\n",
    "    calculate_num_ransac_iterations,\n",
    "    ransac_fundamental_matrix,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic_a = load_image(\n",
    "    \"data/argoverse_log_273c1883/ring_front_center_315975640448534784.jpg\"\n",
    ")\n",
    "scale_a = 0.5\n",
    "pic_b = load_image(\n",
    "    \"data/argoverse_log_273c1883/ring_front_center_315975643412234000.jpg\"\n",
    ")\n",
    "scale_b = 0.5\n",
    "\n",
    "n_feat = 4e3\n",
    "num_matches_to_plot = 50\n",
    "\n",
    "pic_a = cv2.resize(pic_a, None, fx=scale_a, fy=scale_a)\n",
    "pic_b = cv2.resize(pic_b, None, fx=scale_b, fy=scale_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_2d_pic_a, points_2d_pic_b = get_matches(pic_a, pic_b, n_feat)\n",
    "\n",
    "print(\"Found {:d} possibly matching features\".format(len(points_2d_pic_a)))\n",
    "match_image = show_correspondence2(\n",
    "    pic_a,\n",
    "    pic_b,\n",
    "    points_2d_pic_a[:num_matches_to_plot, 0],\n",
    "    points_2d_pic_a[:num_matches_to_plot, 1],\n",
    "    points_2d_pic_b[:num_matches_to_plot, 0],\n",
    "    points_2d_pic_b[:num_matches_to_plot, 1],\n",
    ")\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.imshow(match_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without RANSAC Estimation\n",
    "If we ignore RANSAC and use only our implementation from Part2, we get the following results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_wo_ransac = estimate_fundamental_matrix(points_2d_pic_a, points_2d_pic_b)\n",
    "\n",
    "# Draw epipolar lines using the fundamental matrix\n",
    "draw_epipolar_lines(\n",
    "    F_wo_ransac, pic_a, pic_b, points_2d_pic_a, points_2d_pic_b, figsize=(13, 4)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With Ransac Estimation\n",
    "Now we'll use our RANSAC implementation from Part 3. Where does the epipole fall in the left image? (think about what it represents). The camera is mounted on an autonomous vehicle identical to the vehicle seen up ahead in the left image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F, matched_points_a, matched_points_b = ransac_fundamental_matrix(\n",
    "    points_2d_pic_a, points_2d_pic_b\n",
    ")\n",
    "\n",
    "draw_epipolar_lines(\n",
    "    F, pic_a, pic_b, matched_points_a, matched_points_b, figsize=(13, 4)\n",
    ")\n",
    "\n",
    "match_image = show_correspondence2(\n",
    "    pic_a,\n",
    "    pic_b,\n",
    "    points_2d_pic_a[:num_matches_to_plot, 0],\n",
    "    points_2d_pic_a[:num_matches_to_plot, 1],\n",
    "    points_2d_pic_b[:num_matches_to_plot, 0],\n",
    "    points_2d_pic_b[:num_matches_to_plot, 1],\n",
    ")\n",
    "plt.figure(figsize=(14, 4))\n",
    "plt.imshow(match_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Visual Odometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.vo import get_visual_odometry, plot_poses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following gif, we try to obtain the visual odometry of a camera mounted on a robot from the individual image frames.\n",
    "\n",
    "![VO](https://user-images.githubusercontent.com/16724970/100487935-34fd8b00-30d9-11eb-9941-7735fcae445c.gif \"VO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This may take a few minutes to run across 20 image frames from the Argoverse dataset\n",
    "poses_wTi = get_visual_odometry()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_poses(poses_wTi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e098c37a6bc51c15d5fe2ca7e7edae220231c9af2d0d38bc1af2f83259f282af"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
