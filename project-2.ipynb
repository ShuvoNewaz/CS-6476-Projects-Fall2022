{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [CS 4476/6476 Project 2: Local Feature Matching])\n",
    "\n",
    "This iPython notebook:  \n",
    "(1) Loads and resizes images  \n",
    "(2) Finds interest points in those images                 (you code this)  \n",
    "(3) Describes each interest point with a local feature    (you code this)  \n",
    "(4) Finds matching features                               (you code this)  \n",
    "(5) Visualizes the matches  \n",
    "(6) Evaluates the matches based on ground truth correspondences  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from vision.utils import load_image, PIL_resize, rgb2gray, normalize_img, verify\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Notre Dame\n",
    "image1 = load_image('./data/1a_notredame.jpg')\n",
    "image2 = load_image('./data/1b_notredame.jpg')\n",
    "eval_file = './ground_truth/notredame.pkl'\n",
    "\n",
    "# # Mount Rushmore -- this pair is relatively easy (still harder than Notre Dame, though)\n",
    "# image1 = load_image('./data/2a_rushmore.jpg')\n",
    "# image2 = load_image('./data/2b_rushmore.jpg')\n",
    "# eval_file = './ground_truth/rushmore.pkl'\n",
    "\n",
    "# # Episcopal Gaudi -- This pair is relatively difficult\n",
    "# image1 = load_image('./data/3a_gaudi.jpg')\n",
    "# image2 = load_image('./data/3b_gaudi.jpg')\n",
    "# eval_file = './ground_truth/gaudi.pkl'\n",
    "\n",
    "# # Your own image pair (for part 5) -- replace the name with your file name. Note that there is no eval_file.\n",
    "# image1 = load_image('./data/4a_myimage.jpg')\n",
    "# image2 = load_image('./data/4b_myimage.jpg')\n",
    "\n",
    "scale_factor = 0.5\n",
    "image1 = PIL_resize(image1, (int(image1.shape[1]*scale_factor), int(image1.shape[0]*scale_factor)))\n",
    "image2 = PIL_resize(image2, (int(image2.shape[1]*scale_factor), int(image2.shape[0]*scale_factor)))\n",
    "\n",
    "image1_bw = rgb2gray(image1)\n",
    "image2_bw = rgb2gray(image2)\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image1)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(image2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Part 1: Harris Corner Detector \n",
    "## Find distinctive points in each image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Harris corner detector and SIFT rely heavily upon image gradient information. You'll implement `compute_image_gradients()` and then we'll visualize the magnitude of the image gradients. Which areas have highest mangitude, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_harris_corner import compute_image_gradients\n",
    "from tests.test_part1_harris_corner import test_compute_image_gradients\n",
    "\n",
    "print('compute_image_gradients(): ', verify(test_compute_image_gradients))\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.axis('off')\n",
    "\n",
    "Ix, Iy = compute_image_gradients(image1_bw)\n",
    "gradient_magnitudes = np.sqrt(Ix**2 + Iy**2)\n",
    "gradient_magnitudes = normalize_img(gradient_magnitudes)\n",
    "plt.subplot(1,2,1)\n",
    "plt.title(r'$\\sqrt{I_x^2 + I_y^2}$')\n",
    "plt.imshow( (gradient_magnitudes*255).astype(np.uint8))\n",
    "\n",
    "Ix, Iy = compute_image_gradients(image2_bw)\n",
    "gradient_magnitudes = np.sqrt(Ix**2 + Iy**2)\n",
    "gradient_magnitudes = normalize_img(gradient_magnitudes)\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(r'$\\sqrt{I_x^2 + I_y^2}$')\n",
    "plt.imshow( (gradient_magnitudes*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now compute the second moments $s_x^2, s_y^2, s_x s_y$ at each pixel, which aggregates gradient information in local neighborhoods. We'll use a 2d Gaussian filter to aggregate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_harris_corner import second_moments\n",
    "sx2, sy2, sxsy = second_moments(image1_bw, ksize = 7, sigma = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare $s_x^2$, $s_y^2$, and $s_x s_y$ with $I_x$ and $I_y$, we see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.utils import normalize_img\n",
    "\n",
    "plt.figure(figsize=(12,9))\n",
    "Ix, Iy = compute_image_gradients(image1_bw)\n",
    "plt.subplot(2,3,1); plt.title(r'$I_x$')\n",
    "plt.imshow( (normalize_img(np.abs(Ix))*255).astype(np.uint8))\n",
    "plt.subplot(2,3,2); plt.title(r'$I_y$')\n",
    "plt.imshow( (normalize_img(np.abs(Iy))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(r'$s_x^2$')\n",
    "plt.imshow( (normalize_img(np.abs(sx2))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(r'$s_y^2$')\n",
    "plt.imshow( (normalize_img(np.abs(sy2))*255).astype(np.uint8))\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(r'$s_xs_y$')\n",
    "plt.imshow( (normalize_img(np.abs(sxsy))*255).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $s_xs_y$ is highest where there are both strong x-direction and y-direction gradients (corners and the central rose window).\n",
    "\n",
    "We'll now use these second moments to compute a \"cornerness score\" -- a corner response map -- as a function of these image gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_harris_corner import compute_harris_response_map\n",
    "from tests.test_part1_harris_corner import test_compute_harris_response_map\n",
    "\n",
    "print('compute_harris_response_map(): ', verify(test_compute_harris_response_map))\n",
    "\n",
    "R = compute_harris_response_map(image1_bw)\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(image1_bw, cmap='gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.title(r'$R$')\n",
    "plt.imshow(R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bright areas above are the areas of highest \"corners\".\n",
    "\n",
    "We'll now implement non-max suppression to find local maxima in the 2d response map. One simple way to do non-maximum suppression is to simply pick a local maximum over some window size (u, v). This can be achieved using max-pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to see the maxpool result\n",
    "from vision.part1_harris_corner import maxpool_numpy\n",
    "from tests.test_part1_harris_corner import test_maxpool_numpy, test_nms_maxpool_pytorch\n",
    "from vision.utils import verify\n",
    "\n",
    "# print('maxpool_numpy(): ', verify(test_maxpool_numpy))\n",
    "\n",
    "toy_response_map = np.array(\n",
    "[\n",
    "    [1,2,2,1,2],\n",
    "    [1,6,2,1,1],\n",
    "    [2,2,1,1,1],\n",
    "    [1,1,1,7,1],\n",
    "    [1,1,1,1,1]\n",
    "]).astype(np.float32)\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(toy_response_map.astype(np.uint8))\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "maxpooled_image = maxpool_numpy(toy_response_map, ksize=3)\n",
    "plt.imshow(maxpooled_image.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a simple 5x5 grid of response scores, non-max suppression will allow us to choose values that are local maxima. If we request the top $k=2$ responses of the toy response grid above, we should get (1,1) and (3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_harris_corner import nms_maxpool_pytorch\n",
    "\n",
    "print('nms_maxpool_pytorch(): ', verify(test_nms_maxpool_pytorch))\n",
    "\n",
    "x_coords, y_coords, confidences = nms_maxpool_pytorch(toy_response_map, k=2, ksize=3)\n",
    "print('Coordinates of local maxima:')\n",
    "for x, y, c in zip(x_coords, y_coords, confidences):\n",
    "    print(f'\\tAt {x},{y}, local maximum w/ confidence={c:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will call the `get_harris_interest_points()` function in `part1_harris_corner.py` to detect 'interesting' points in the images. \n",
    "\n",
    "**IMPORTANT**\n",
    "Make sure to add your code in the `get_harris_interest_points()` function to call Harris Corner Detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part1_harris_corner import test_get_harris_interest_points, test_remove_border_vals\n",
    "\n",
    "# print('test_remove_border_vals(): ', verify(test_remove_border_vals))\n",
    "\n",
    "print('get_harris_interest_points()', verify(test_get_harris_interest_points))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "from vision.part1_harris_corner import get_harris_interest_points\n",
    "from vision.utils import show_interest_points\n",
    "\n",
    "num_interest_points = 2500\n",
    "X1, Y1, _ = get_harris_interest_points( copy.deepcopy(image1_bw), num_interest_points)\n",
    "X2, Y2, _ = get_harris_interest_points( copy.deepcopy(image2_bw), num_interest_points)\n",
    "\n",
    "num_pts_to_visualize = 300\n",
    "# Visualize the interest points\n",
    "rendered_img1 = show_interest_points(image1, X1[:num_pts_to_visualize], Y1[:num_pts_to_visualize])\n",
    "rendered_img2 = show_interest_points(image2, X2[:num_pts_to_visualize], Y2[:num_pts_to_visualize])\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1); plt.imshow(rendered_img1)\n",
    "plt.subplot(1,2,2); plt.imshow(rendered_img2)\n",
    "print(f'{len(X1)} corners in image 1, {len(X2)} corners in image 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Normalized Patch Feature Descriptor\n",
    "## Create feature vectors at each interest point (Szeliski 7.1.2)\n",
    "Perhaps the simplest possible keypoint descriptor is to stack the 16x16 patch surrounding the keypoint into a 256-dimensional vector, and normalize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part2_patch_descriptor import compute_normalized_patch_descriptors\n",
    "from tests.test_part2_patch_descriptor import test_compute_normalized_patch_descriptors\n",
    "\n",
    "print('compute_normalized_patch_descriptors:', verify(test_compute_normalized_patch_descriptors))\n",
    "\n",
    "image1_features = compute_normalized_patch_descriptors(image1_bw, X1, Y1, feature_width=16)\n",
    "image2_features = compute_normalized_patch_descriptors(image2_bw, X2, Y2, feature_width=16)\n",
    "\n",
    "# Visualize what the first 300 feature vectors for image 1 look like (they should not be identical or all black)\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1); plt.imshow(image1_features[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Match features (Szeliski 7.1.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test your feature matching implementation\n",
    "from tests.test_part3_feature_matching import (\n",
    "    test_match_features_ratio_test,\n",
    "    test_compute_feature_distances_2d,\n",
    "    test_compute_feature_distances_10d\n",
    ")\n",
    "print('compute_feature_distances (2d):', verify(test_compute_feature_distances_2d))\n",
    "print('compute_feature_distances (10d):', verify(test_compute_feature_distances_10d))\n",
    "print('match_features_ratio_test:', verify(test_match_features_ratio_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part3_feature_matching import match_features_ratio_test\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(X1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "You might want to set 'num_pts_to_visualize' and 'num_pts_to_evaluate' to some constant (e.g. 100) once you start detecting hundreds of interest points, otherwise things might get too cluttered. You could also threshold based on confidence.  \n",
    "  \n",
    "There are two visualization functions below. You can comment out one of both of them if you prefer.\n",
    "\n",
    "NOTE: If you find that no matches are returned, you may encounter an error in the cells below. To avoid this, adjust your threshold in `match_features_ratio_test` to include at least one match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from vision.utils import show_correspondence_circles, show_correspondence_lines\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 200\n",
    "c1 = show_correspondence_circles(image1, image2,\n",
    "                    X1[matches[:num_pts_to_visualize, 0]], Y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    X2[matches[:num_pts_to_visualize, 1]], Y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c1)\n",
    "plt.savefig('./results/vis_circles.jpg', dpi=1000)\n",
    "c2 = show_correspondence_lines(image1, image2,\n",
    "                    X1[matches[:num_pts_to_visualize, 0]], Y1[matches[:num_pts_to_visualize, 0]],\n",
    "                    X2[matches[:num_pts_to_visualize, 1]], Y2[matches[:num_pts_to_visualize, 1]])\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c2)\n",
    "plt.savefig('./results/vis_lines.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment out the function below if you are not testing on the Notre Dame, Episcopal Gaudi, and Mount Rushmore image pairs--this evaluation function will only work for those which have ground truth available.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.utils import evaluate_correspondence\n",
    "# num_pts_to_evaluate = len(matches)\n",
    "num_pts_to_evaluate = 2500\n",
    "_, c = evaluate_correspondence(image1, image2, eval_file, scale_factor,\n",
    "                        X1[matches[:num_pts_to_evaluate, 0]], Y1[matches[:num_pts_to_evaluate, 0]],\n",
    "                        X2[matches[:num_pts_to_evaluate, 1]], Y2[matches[:num_pts_to_evaluate, 1]])\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "plt.savefig('./results/eval.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Sift Feature Descriptor (Szeliski 7.1.2)\n",
    "SIFT relies upon computing the magnitudes and orientations of image gradients, and then computing weighted histograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part4_sift_descriptor import (\n",
    "    test_get_magnitudes_and_orientations,\n",
    "    test_get_gradient_histogram_vec_from_patch\n",
    ")\n",
    "print('get_magnitudes_and_orientations:', verify(test_get_magnitudes_and_orientations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('get_gradient_histogram_vec_from_patch():', verify(test_get_gradient_histogram_vec_from_patch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part4_sift_descriptor import test_get_feat_vec, test_get_SIFT_descriptors\n",
    "print(verify(test_get_feat_vec))\n",
    "print(verify(test_get_SIFT_descriptors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from vision.part4_sift_descriptor import get_SIFT_descriptors\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "image1_features = get_SIFT_descriptors(image1_bw, X1, Y1)\n",
    "image2_features = get_SIFT_descriptors(image2_bw, X2, Y2)\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print(f'SIFT took {duration} sec.')\n",
    "\n",
    "# visualize what the values of the first 200 SIFT feature vectors look like (should not be identical or all black)\n",
    "plt.figure(); plt.subplot(1,2,1); plt.imshow(image1_features[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from vision.utils import show_correspondence_circles, show_correspondence_lines\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "print('{:d} matches from {:d} corners'.format(len(matches), len(X1)))\n",
    "\n",
    "# num_pts_to_visualize = len(matches)\n",
    "num_pts_to_visualize = 200\n",
    "c1 = show_correspondence_circles(\n",
    "    image1,\n",
    "    image2,\n",
    "    X1[matches[:num_pts_to_visualize, 0]],\n",
    "    Y1[matches[:num_pts_to_visualize, 0]],\n",
    "    X2[matches[:num_pts_to_visualize, 1]],\n",
    "    Y2[matches[:num_pts_to_visualize, 1]]\n",
    ")\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c1)\n",
    "plt.savefig('./results/vis_circles.jpg', dpi=1000)\n",
    "c2 = show_correspondence_lines(\n",
    "    image1,\n",
    "    image2,\n",
    "    X1[matches[:num_pts_to_visualize, 0]],\n",
    "    Y1[matches[:num_pts_to_visualize, 0]],\n",
    "    X2[matches[:num_pts_to_visualize, 1]],\n",
    "    Y2[matches[:num_pts_to_visualize, 1]]\n",
    ")\n",
    "plt.figure(figsize=(10,5)); plt.imshow(c2)\n",
    "plt.savefig('./results/vis_lines.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.utils import evaluate_correspondence\n",
    "num_pts_to_evaluate = len(matches)\n",
    "_, c = evaluate_correspondence(\n",
    "    image1,\n",
    "    image2,\n",
    "    eval_file,\n",
    "    scale_factor,\n",
    "    X1[matches[:num_pts_to_evaluate, 0]],\n",
    "    Y1[matches[:num_pts_to_evaluate, 0]],\n",
    "    X2[matches[:num_pts_to_evaluate, 1]],\n",
    "    Y2[matches[:num_pts_to_evaluate, 1]]\n",
    ")\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "plt.savefig('./results/eval.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure your code runs in under 90 sec and achieves >80% acc on the Notre Dame pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part4_sift_descriptor import (\n",
    "    test_feature_matching_speed,\n",
    "    test_feature_matching_accuracy\n",
    ")\n",
    "print('SIFT pipeline speed test:', verify(test_feature_matching_speed))\n",
    "print('SIFT pipeline accuracy test:', verify(test_feature_matching_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional Extra Credit\n",
    "Implement SIFT in a fully vectorized manner with at least 80% accuracy on Notre Dame (With no loops over pixels, but at most a loop over keypoints). SIFT on both images should run in under 5 seconds combined. Orientation at each vector can be obtained via 1x1 convolution with 8 unit vector basis vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from vision.part4_sift_descriptor import get_sift_features_vectorized\n",
    "\n",
    "start = time.time()\n",
    "image1_features = get_sift_features_vectorized(image1_bw, X1, Y1)\n",
    "image2_features = get_sift_features_vectorized(image2_bw, X2, Y2)\n",
    "end = time.time()\n",
    "duration = end - start\n",
    "print(f'SIFT took {duration} sec.')\n",
    "\n",
    "matches, confidences = match_features_ratio_test(image1_features, image2_features)\n",
    "\n",
    "num_pts_to_evaluate = len(matches) - 1\n",
    "_, c = evaluate_correspondence(\n",
    "    image1,\n",
    "    image2,\n",
    "    eval_file,\n",
    "    scale_factor,\n",
    "    X1[matches[:, 0]],\n",
    "    Y1[matches[:, 0]],\n",
    "    X2[matches[:, 1]],\n",
    "    Y2[matches[:, 1]]\n",
    ")\n",
    "plt.figure(figsize=(8,4)); plt.imshow(c)\n",
    "plt.savefig('./results/eval.jpg', dpi=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part4_sift_descriptor import test_extra_credit_vectorized_sift\n",
    "\n",
    "print(verify(test_extra_credit_vectorized_sift))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cv_proj2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e714cf29c05812e46cdf818068b9fa17ea680e67df41450dee73034d677ccbf"
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
