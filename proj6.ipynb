{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Project 6: Classifying Point Clouds with PointNet](https://github.gatech.edu/cs4476/project-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we are going to build deep learning architectures to classify point clouds that were extracted from self-driving car LiDAR scans.\n",
    "\n",
    "Basic learning objectives of this project:\n",
    "* Construct the basic pipeline used for point cloud classification\n",
    "* Analyze the results from our model and look for potential points of improvement\n",
    "* Understand the limitations of the model and improve it using positional encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from vision.utils import verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a second to visualize a few point clouds that we are going to train our model on. You might need to rotate the visualization before you can actually see the object being represented by the points. See if you can make out this pedestrian's arms, legs, and head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.utils import plot_cloud_from_file\n",
    "\n",
    "plot_cloud_from_file('data/sweeps/PEDESTRIAN/11.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first order of business we're going to take care of is writing a PyTorch dataset that will load in the data we'll be using to train and test our model. We have a total of 20 different classes that all found under the `data/sweeps` folder and each class has been split into a training and testing set. We will write a class that will load the appropriate data given the split we pass in (i.e. train/test split).\n",
    "\n",
    "**TODO 1:** complete `part1_dataloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part1_dataloader import Argoverse\n",
    "from torch.utils.data import DataLoader\n",
    "from tests.test_part1 import (\n",
    "    test_dataset_length, \n",
    "    test_unique_values, \n",
    "    test_get_points_from_file, \n",
    "    test_pad_points, \n",
    "    test_class_values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_dataset_length(): \", verify(test_dataset_length))\n",
    "print(\"test_unique_values(): \", verify(test_unique_values))\n",
    "print(\"test_get_points_from_file(): \", verify(test_get_points_from_file))\n",
    "print(\"test_pad_points(): \", verify(test_pad_points))\n",
    "print(\"test_class_values(): \", verify(test_class_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load in our data. When extracting object point clouds from LiDAR scans, we pruned point clouds with large numbers of points to reduce the computational demand of training and testing our classifiers. We pruned all point clouds to have a maximum of 200 points so that is the size that we will be padding all of the other point clouds to. (Feel free to adjust the batch size if you find a value that works better with your implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Argoverse('train', 'data/sweeps', 200)\n",
    "test_dataset = Argoverse('test', 'data/sweeps', 200)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, num_workers=4, persistent_workers=True, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, num_workers=4, persistent_workers=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Baseline\n",
    "\n",
    "A very simple baseline to start with is a voxel-based approach. Our overall point cloud falls in a 4x4x4 meter volume in space. We can divide this up into 1x1x1 meter volumes which would give us a total of 64 $1m^3$ cells. We build a 64-dimensional feature for a point by simply counting how many points in the point cloud fall into each cell then normalizng the resulting feature vector.\n",
    "\n",
    "For more details see the project pdf.\n",
    "\n",
    "**TODO 2:** complete `part2_baseline.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part2_baseline import Baseline\n",
    "from torch import optim\n",
    "from vision.training import train, test\n",
    "from tests.test_part2 import (\n",
    "    test_count_points,\n",
    "    test_baseline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_count_points(): \", verify(test_count_points))\n",
    "print(\"test_baseline(): \", verify(test_baseline))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets train our baseline implementation and see how it does. Fill in the hyperparameters and train your model.\n",
    "\n",
    "To get full credit for this section, you will need *$\\geq$ 30%* accuracy on the test data set. The TAs were able to consistently achieve this accuracy with ~15 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = None\n",
    "weight_decay = None\n",
    "epochs = None\n",
    "\n",
    "model = Baseline(20)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, train_loss, test_acc, test_loss = train(model, optimizer, epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('output/Baseline.pt')\n",
    "test_acc, test_loss = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: PointNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a baseline to compare to, we're going to build a simplified version of PointNet which was first published in 2017 by researchers at Stanford University. You can view the original website for this publication [here](https://stanford.edu/~rqi/pointnet/). \n",
    "\n",
    "We will be slightly modifying the original architecture. The architecture described in the PointNet publication includes two modules that we will not be implementing. Namely, they are the input transform and the feature transform. We will also not be using our PointNet implementation for semantic segmentation so you can ignore that part of the architecture as well. For an architecture diagram of what we will be implementing, check the project pdf.\n",
    "\n",
    "**TODO 3**: complete `part3_pointnet.py`, use the architecture diagram in the project pdf for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tests.test_part3 import test_pointnet\n",
    "from vision.part3_pointnet import PointNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_pointnet(): \", verify(test_pointnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a PointNet implementation, lets train our model and see how it does. Fill in the hyper-parameters and give your new model a spin! \n",
    "\n",
    "To get full credit for this section, you will need *$\\geq$ 65% accuracy* on the test dataset. The TAs were able to consistently achieve this accuracy with ~15 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = None\n",
    "weight_decay = None\n",
    "epochs = None\n",
    "\n",
    "model = PointNet(classes=20)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, train_loss, test_acc, test_loss = train(model, optimizer, epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('output/PointNet.pt')\n",
    "test_acc, test_loss = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Performance Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained PointNet model, we can look at how it performs and what its shortcomings are. We are also going to analyze our point clouds to see which parts of them the model deemed to be most important in making its classification decision.\n",
    "\n",
    "**TODO 4:** complete `part4_analysis.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.utils import (\n",
    "    generate_and_plot_confusion_matrix,\n",
    "    plot_crit_points_from_file\n",
    ")\n",
    "from tests.test_part4 import (\n",
    "    test_critical_indices,\n",
    "    test_critical_indices_with_duplicates,\n",
    "    test_confusion_matrix,\n",
    "    test_confusion_matrix_normalized\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_critical_indices(): \", verify(test_critical_indices))\n",
    "print(\"test_critical_indices_with_duplicates(): \", verify(test_critical_indices_with_duplicates))\n",
    "print(\"test_confusion_matrix(): \", verify(test_confusion_matrix))\n",
    "print(\"test_confusion_matrix_normalized(): \", verify(test_confusion_matrix_normalized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets use the confusion matrix to look at which mistakes our model is making most often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_plot_confusion_matrix(model, test_loader, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets visualize the critical points calculated by our model for the predestrian clouds from before. Recall that these are the points that contributed to the global feature. Why do you think these were the critical points the model ended up using? (Feel free to use different point clouds for your analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_crit_points_from_file(model, 'data/sweeps/PEDESTRIAN/11.txt', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Positional Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our basic PointNet model does pretty well. But let's see if we can do any better. One proposed solution to improve on the shortcomings of the basic PointNet model is using positional encoding (see pdf for more details). \n",
    "\n",
    "**TODO 5:** complete `part5_positional_encoding.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision.part5_positional_encoding import PointNetPosEncoding\n",
    "from tests.test_part5 import (\n",
    "    test_positional_encoding,\n",
    "    test_pointnet_with_positional_encoding\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test_positional_encoding(): \", verify(test_positional_encoding))\n",
    "print(\"test_pointnet_with_positional_encoding(): \", verify(test_pointnet_with_positional_encoding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check out how our model performs now that we added positional encoding.\n",
    "\n",
    "To get full credit for this section, you will need *$\\geq$ 65% accuracy* on the test dataset. The TAs were able to consistently achieve this accuracy with ~10 epochs of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = None\n",
    "weight_decay = None\n",
    "epochs = None\n",
    "\n",
    "model = PointNetPosEncoding(classes=20)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc, train_loss, test_acc, test_loss = train(model, optimizer, epochs, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('output/PointNetPosEncoding.pt')\n",
    "test_acc, test_loss = test(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets take a look at how the addition of positional encoding improved on the performance of our original PointNet model. Which classes does the positional encoding help the most with? Which ones does it not really seem to make much of a difference for? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_plot_confusion_matrix(model, test_loader, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_crit_points_from_file(model, 'data/sweeps/PEDESTRIAN/11.txt', 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "74396501bd377449ed34cc27f22eec8ef2c76b2ba7fe805dab1a9b94c2831a5b"
   }
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
