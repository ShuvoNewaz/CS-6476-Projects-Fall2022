{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e92b9c64",
   "metadata": {},
   "source": [
    "# [Semantic Segmentation with Deep Learning](https://www.cc.gatech.edu/~hays/compvision/proj6/)\n",
    "\n",
    "For this project we are going to focus on semantic segmentation for 11 semantic categories with a state-of-the-art approach: deep learning.\n",
    "\n",
    "Basic learning objectives of this project:\n",
    "\n",
    "1. Understanding the ResNet architecture.\n",
    "2. Understand the concepts behind data augmentation and learning rate schedules for semantic segmentation\n",
    "3. Understand the role of dilated convolution and context in increasing the receptive field of a network.\n",
    "4. Experiment with different aspects of the training process and observe the performance.\n",
    "\n",
    "The starter code is mostly initialized to 'placeholder' just so that the starter code does not crash when run unmodified and you can get a preview of how results are presented.\n",
    "\n",
    "Your trained model should be able to produce an output like the one shown on the right below:\n",
    "\n",
    "Camvid Image | Model Prediction\n",
    ":-: | :--:\n",
    "<img src=\"https://user-images.githubusercontent.com/16724970/114431741-d6b7dd00-9b8d-11eb-8822-e7fa7e915e37.jpg\" width=\"300\"> | <img src=\"https://user-images.githubusercontent.com/16724970/114431739-d61f4680-9b8d-11eb-9266-e56aeb08476f.jpg\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee21d0a",
   "metadata": {},
   "source": [
    "## PSPNet and ResNet-50\n",
    "\n",
    "We'll be implementing PSPNet for this project, which uses a ResNet-50 backbone. ResNet-50 has 50 convolutional layers, which is significantly deeper than your SimpleNet of Project 5. We give you the implementation in `src/vision/resnet.py`. \n",
    "\n",
    "The ResNet-50 is composed of 4 different sections (each called a \"layer\"), named `layer1`, `layer2`, `layer3`, `layer4`. Each layer is composed of a repeated number of blocks, and each such block is named a `BottleNeck`. Specifically, `layer1` has 3 Bottlenecks, `layer2` has 4 Bottlenecks, `layer3` has 6 Bottlenecks, and `layer4` has 3 Bottlenecks. In all, ResNet-50 has 16 Bottlenecks, which accounts for 48 of the conv layers.\n",
    "\n",
    "### Visualizing a ResNet Bottleneck Module\n",
    "\n",
    "The BottleNeck has a residual connection, from which ResNet gets its name:\n",
    "\n",
    "<img width=\"300\" src=\"https://user-images.githubusercontent.com/16724970/114430171-2ac1c200-9b8c-11eb-8341-fc943ff0945f.png\">\n",
    "\n",
    "See Figure 5 of the [ResNet paper](https://arxiv.org/pdf/1512.03385.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb47f0d",
   "metadata": {},
   "source": [
    "### Implementing a Bottleneck\n",
    "\n",
    "The Bottleneck is implemented exactly as the figure above shows, with 1x1 Conv -> BN -> ReLU -> 3x3 Conv -> BN -> ReLU -> 1x1 Conv -> BN -> Optional Downsample -> Add Back Input -> ReLU. The channel dimension of the feature map will be expanded by 4x, as we can see by the conv layer `in_features` and `out_features` parameters. And notice that the stride is set at the `conv2` module, which will be very important later.\n",
    "\n",
    "```python\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "```\n",
    "\n",
    "and the forward method of the `Bottleneck` shows the residual connection. Notice that when we add back the input (the identity operation), we may need to downsample it for the shapes to match during the add operation (if the main branch downsampled the input):\n",
    "```python\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6814b13d",
   "metadata": {},
   "source": [
    "## Visualizing the Architecture\n",
    "Plotting the whole network architecture would require a massive figure, but we can show how data flows through just one Bottleneck, starting with 64 channels, and ending up with 256 output channels:\n",
    "<p float=\"left\">\n",
    "  <img src=\"https://user-images.githubusercontent.com/16724970/114427960-9eae9b00-9b89-11eb-9a3b-96817f205f32.png\" width=\"400\" />\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f87188",
   "metadata": {},
   "source": [
    "## Part 1: Pyramid Pooling Module\n",
    "In Part 1, you will implement the Pyramid Pooling Module (PPM). After feeding an image through the ResNet backbone and obtaining a feature map, PSPNet aggregates context over different portions of the image with the PPM.\n",
    "\n",
    "The PPM splits the $H \\times W$ feature map into KxK grids. Here, 1x1, 2x2, 3x3,and 6x6 grids are formed, and features are average-pooled within each grid cell. Afterwards, the 1x1, 2x2, 3x3, and 6x6 grids are upsampled back to the original $H \\times W$ feature map resolution, and are stacked together along the channel dimension. These grids are visualized below (center):\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/16724970/114436422-4b414a80-9b93-11eb-8f02-8e7506b5f9a1.jpg\" width=\"900\">\n",
    "\n",
    "Implement this in `src/vision/part1_ppm.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8cec2d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_PPM_6x6():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_PPM_fullres():  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from tests.test_part1_ppm import test_PPM_6x6, test_PPM_fullres\n",
    "from src.vision.utils import verify\n",
    "\n",
    "print(\"test_PPM_6x6(): \", verify(test_PPM_6x6))\n",
    "print(\"test_PPM_fullres(): \", verify(test_PPM_fullres))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6597f6",
   "metadata": {},
   "source": [
    "## Part 2: Dataset and Dataloader\n",
    "Next, in `src/vision/part2_dataset.py` you will implement the `make_dataset()` functions to create a list of paths to (image, ground truth) pairs. You will also implement the `__getitem__()` function that will load an RGB image and grayscale label map, and then apply a transform to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a4a9103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of (image,label) pairs train list generated!\n",
      "test_SemData_len():  \u001b[32m\"Correct\"\u001b[0m\n",
      "List of (image,label) pairs train list generated!\n",
      "test_getitem_no_data_aug():  \u001b[32m\"Correct\"\u001b[0m\n",
      "List of (image,label) pairs train list generated!\n",
      "test_make_dataset():  \u001b[31m\"Wrong\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests.test_part2_dataset import test_SemData_len, test_getitem_no_data_aug, test_make_dataset\n",
    "\n",
    "print(\"test_SemData_len(): \", verify(test_SemData_len))\n",
    "print(\"test_getitem_no_data_aug(): \", verify(test_getitem_no_data_aug))\n",
    "print(\"test_make_dataset(): \", verify(test_make_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3752c5",
   "metadata": {},
   "source": [
    "## Part 3: Online Data Preprocessing and Data Augmentation\n",
    "Data preprocessing and augmentation is very important to good performance, and we'll implement this in `src/vision/part3_training_utils.py`. We'll feed in square image crops to the network, but we must be careful to crop the same portion of the RGB image and ground truth semantic label map. Implement `get_train_transform(args)` and `get_val_transform(args)`, and check against the unit tests below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ab36c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_get_train_transform():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_get_val_transform():  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests.test_part3_training_utils import test_get_train_transform, test_get_val_transform\n",
    "\n",
    "print(\"test_get_train_transform(): \", verify(test_get_train_transform))\n",
    "print(\"test_get_val_transform(): \", verify(test_get_val_transform))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06ef17e",
   "metadata": {},
   "source": [
    "## Part 4: A Simple Segmentation Baseline\n",
    "We'll start with a very simple baseline -- a pretrained ResNet-50, without the final averagepool/fc layer, and a single 1x1 conv as a final classifier, converting the (2048,7,7) feature map to scores over 11 classes, a (11,7,7) tensor. Note that our output is just 7x7, which is very low resolution. Implement upsampling to the original height and width, and compute the loss and predicted class per pixel in `src/vision/part4_segmentation_net.py`.\n",
    "\n",
    "If the \"SimpleSegmentationNet\" architecture is specified in the experiment arguments (`args`), return this model in `get_model_and_optimizer()` in `part3_training_utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d5533da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_check_output_shapes():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_check_output_shapes_testtime():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_get_model_and_optimizer_simplearch():  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests.test_part4_segmentation_net import (\n",
    "    test_check_output_shapes,\n",
    "    test_check_output_shapes_testtime,\n",
    "    test_get_model_and_optimizer_simplearch\n",
    ")\n",
    "\n",
    "print(\"test_check_output_shapes(): \", verify(test_check_output_shapes))\n",
    "print(\"test_check_output_shapes_testtime(): \", verify(test_check_output_shapes_testtime))\n",
    "print(\"test_get_model_and_optimizer_simplearch(): \", verify(test_get_model_and_optimizer_simplearch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa5cf7",
   "metadata": {},
   "source": [
    "## Part 5: Net Surgery for Increased Output Resolution and Receptive Field\n",
    "The basic ResNet-50 has two major problems:\n",
    "1. It does not have a large enough receptive field\n",
    "2. If run fully-convolutionally, it produces a low-resolution output (just $7 \\times 7$)!\n",
    "\n",
    "To fix the first problem, will need to replace some of its convolutional layers with dilated convolution. To fix the second problem, we'll reduce the stride of the network from 2 to 1, so that we don't downsample so much. Instead of going down to 7x7, we'll reduce to 28x28 for 224x224 input, or 26x26 for 201x201, like we do in this project. In other words, the downsampling rate will go from (1/32) to just (1/8).\n",
    "\n",
    "These animations depict how the dilated convolution (i.e. with dilation > 1) operation compares to convolution with no dilation (i.e. with dilation=1).\n",
    "\n",
    "Conv w/ Stride=1, Dilation=1 | Conv w/ Stride=2, Dilation=1 | Conv w/ Stride=1, Dilation=2\n",
    ":-: | :-: | :-:\n",
    "<img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_no_strides.gif\" width=\"300\" align=\"center\"> | <img src=\"https://raw.githubusercontent.com/vdumoulin/conv_arithmetic/master/gif/no_padding_strides.gif\" width=\"300\" align=\"center\"> | <img src=\"https://github.com/vdumoulin/conv_arithmetic/raw/master/gif/dilation.gif\" width=\"300\" align=\"center\"> \n",
    "\n",
    "\n",
    "In Layer3, in every `Bottleneck`, we will change the 3x3 `conv2`, we will replace the conv layer that had stride=2, dilation=1, and padding=1 with a new conv layer, that instead  has stride=1, dilation=2, and padding=2. In the `downsample` block, we'll also need to hardcode the stride to 1, instead of 2.\n",
    "\n",
    "In Layer4, for every `Bottleneck`, we will make the same changes, except we'll change the dilation to 4 and padding to 4.\n",
    "\n",
    "Make these edits in `src/vision/part5_pspnet.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6a97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_pspnet_output_shapes(): \u001b[32m\"Correct\"\u001b[0m\n",
      "test_check_output_shapes_testtime_pspnet():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_check_output_shapes_zoom_factor_testtime_pspnet():  \u001b[32m\"Correct\"\u001b[0m\n",
      "test_get_model_and_optimizer_pspnet():  \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests.test_part5_pspnet import (\n",
    "    test_pspnet_output_shapes,\n",
    "    test_check_output_shapes_testtime_pspnet,\n",
    "    test_get_model_and_optimizer_pspnet,\n",
    "    test_pspnet_output_with_zoom_factor\n",
    ")\n",
    "\n",
    "print(\"test_pspnet_output_shapes():\", verify(test_pspnet_output_shapes))\n",
    "print(\"test_check_output_shapes_testtime_pspnet(): \", verify(test_check_output_shapes_testtime_pspnet))\n",
    "print(\"test_check_output_shapes_zoom_factor_testtime_pspnet(): \", verify(test_pspnet_output_with_zoom_factor))\n",
    "\n",
    "print(\"test_get_model_and_optimizer_pspnet(): \", verify(test_get_model_and_optimizer_pspnet))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7721b0c",
   "metadata": {},
   "source": [
    "## Part 6 Transfer Learning\n",
    "\n",
    "This section is required for CS 6476 students and optional for CS 4476.\n",
    "\n",
    "Use the model trained on Camvid as a pretrained model, and train it on Kitti Dataset. The Kitti dataloader is provided. Finish the model_and_optimizer function in part 6.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2461a35c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_model_kitti(): \u001b[32m\"Correct\"\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from tests.test_part6_kitti_dataset import test_model_kitti\n",
    "print(\"test_model_kitti():\", verify(test_model_kitti))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cv_proj5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "7513e212a8bb9e7ecd9258347352e99d61ea78a5d7a748fe56661332f467eef8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
